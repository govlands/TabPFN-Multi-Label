import numpy as np
from sklearn.datasets import make_multilabel_classification
from sklearn.model_selection import train_test_split
from sklearn.multioutput import MultiOutputClassifier, ClassifierChain
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier
from tabpfn import TabPFNClassifier
from sklearn.metrics import f1_score, hamming_loss, roc_auc_score, accuracy_score
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from mlp_att import evaluate_multilabel, train, predict, MLPWithAttention, load_model, get_dataset
from utils import apply_thresholds, optimize_thresholds
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
from matplotlib.patches import Polygon
from datetime import datetime

def classes_list_from_chain(chain, n_labels):
    """
    è¿”å›é•¿åº¦ä¸º n_labels çš„åˆ—è¡¨ï¼Œç´¢å¼•å¯¹åº”åŸå§‹æ ‡ç­¾åˆ—ï¼Œ
    æ¯é¡¹ä¸ºè¯¥æ ‡ç­¾å¯¹åº”å­åˆ†ç±»å™¨çš„ classes_ï¼ˆæˆ–è€… Noneï¼‰ã€‚
    """
    classes_list = [None] * n_labels
    if not hasattr(chain, "estimators_"):
        return classes_list

    # chain.order_ ç»™å‡º estimators_ å¯¹åº”çš„åŸå§‹æ ‡ç­¾ç´¢å¼•é¡ºåº
    order = getattr(chain, "order_", None)
    if order is None:
        # sklearn æ—§ç‰ˆæœ¬å¯èƒ½æ²¡æœ‰ order_ï¼Œå‡å®šæŒ‰ 0..n_labels-1
        order = np.arange(n_labels)

    for est, lbl_idx in zip(chain.estimators_, order):
        classes_list[int(lbl_idx)] = getattr(est, "classes_", None)
    return classes_list

def formalize_output_probas(probas, n_labels, classes_list=None, positive_label=1):
    """
    æŠŠ predict_proba çš„è¾“å‡ºç»Ÿä¸€æˆ (n_samples, n_labels) çš„æ­£ç±»æ¦‚ç‡çŸ©é˜µã€‚
    ä¼˜å…ˆä½¿ç”¨ classes_listï¼ˆæ¯ä¸ªæ ‡ç­¾å¯¹åº”çš„ classes_ï¼‰æ¥å®šä½æ­£ç±»åˆ—ã€‚
    å‚æ•°:
      - probas: predict_proba çš„è¿”å›å€¼ï¼ˆlist / ndarray (2D/3D)ï¼‰
      - n_labels: æœŸæœ›æ ‡ç­¾æ•°
      - classes_list: å¯é€‰ listï¼Œæ¯é¡¹ä¸ºè¯¥æ ‡ç­¾å¯¹åº”çš„ classes_ï¼ˆä¾‹å¦‚ estimator.classes_ï¼‰
      - positive_label: å¸Œæœ›å–ä¸ºæ­£ç±»çš„æ ‡ç­¾å€¼ï¼ˆé»˜è®¤ 1ï¼‰
    è¿”å›:
      - numpy.ndarray, shape (n_samples, n_labels)
    """

    def pick_pos_col(arr2d, idx):
        arr = np.asarray(arr2d)
        # ä¸€ç»´æˆ–å•åˆ—ç›´æ¥å±•å¹³
        if arr.ndim == 1:
            return arr.ravel()
        if arr.ndim == 2:
            # å¦‚æœä¼ å…¥äº† classes_listï¼Œåˆ™ä¼˜å…ˆç”¨å®ƒå®šä½åˆ—
            if classes_list is not None and idx < len(classes_list) and classes_list[idx] is not None:
                cls = np.asarray(classes_list[idx], dtype=object)
                # åŒ¹é…æ•°å­—æˆ–å­—ç¬¦ä¸²å½¢å¼
                matches = np.where((cls == positive_label) | (cls.astype(str) == str(positive_label)))[0]
                if matches.size:
                    col = matches[0]
                    return arr[:, col]
            # å›é€€ï¼šäºŒåˆ†ç±»é»˜è®¤å–åˆ— 1ï¼ˆå¸¸è§ classes_ == [0,1]ï¼‰
            if arr.shape[1] > 1:
                return arr[:, 1]
            # å•åˆ—æƒ…å†µ
            return arr[:, 0]
        raise ValueError(f"Unsupported array ndim for single-label proba: {arr.ndim}")

    # 1) list of arrays (one per label)
    if isinstance(probas, list):
        cols = []
        for i, p in enumerate(probas):
            p = np.asarray(p)
            cols.append(pick_pos_col(p, i))
        return np.column_stack(cols)

    probas = np.asarray(probas)

    # 2) possible shape (n_labels, n_samples, n_classes)
    if probas.ndim == 3:
        n_labels0 = probas.shape[0]
        cols = []
        for i in range(n_labels0):
            arr2d = probas[i]
            cols.append(pick_pos_col(arr2d, i))
        return np.column_stack(cols)

    # 3) possible shape (n_samples, n_labels) already
    if probas.ndim == 2:
        if probas.shape[1] == n_labels:
            return probas
        # æœ‰æ—¶è¿”å› (n_samples, n_labels * n_classes) â€”â€” ä¸æ”¯æŒè‡ªåŠ¨æ‹†åˆ†
        raise ValueError("äºŒç»´ predict_proba è¾“å‡ºçš„åˆ—æ•°ä¸ n_labels ä¸åŒ¹é…ï¼›è¯·æä¾› classes_list æˆ–æ£€æŸ¥åŸºå­¦ä¹ å™¨ã€‚")

    raise ValueError("æ— æ³•è¯†åˆ« predict_proba è¾“å‡ºæ ¼å¼ï¼Œè¯·æ£€æŸ¥ base estimator æ˜¯å¦æ”¯æŒ predict_probaã€‚")

class MLPBaseline(nn.Module):
    def __init__(self, n_features, n_labels, hidden=(256, 128), p_drop=0.2, use_layernorm=True):
        super().__init__()
        layers = []
        d_prev = n_features
        for d_h in hidden:
            layers.append(nn.Linear(d_prev, d_h))
            layers.append(nn.LayerNorm(d_h) if use_layernorm else nn.BatchNorm1d(d_h))
            layers.append(nn.GELU())
            layers.append(nn.Dropout(p_drop))
            d_prev = d_h
        self.backbone = nn.Sequential(*layers)
        self.head = nn.Linear(d_prev, n_labels)
        
    def forward(self, X):
        # X:(batch, n_features)
        z = self.backbone(X)
        return self.head(z)

def visualize_model_comparison(results_dict, save_prefix=None, figsize=(16, 10)):
    """
    å¯¹æ¯”å¤šä¸ªæ¨¡å‹çš„æ€§èƒ½æŒ‡æ ‡ï¼Œåˆ†ç»„æ˜¾ç¤ºå¹¶ä¿å­˜å¤šä¸ªå›¾åƒæ–‡ä»¶
    
    å‚æ•°:
        results_dict: dictï¼Œæ ¼å¼ {model_name: {metric_name: value, ...}, ...}
        save_prefix: strï¼Œä¿å­˜å›¾åƒçš„å‰ç¼€è·¯å¾„ï¼ˆå¯é€‰ï¼‰
        figsize: tupleï¼Œæ¯ç»„å›¾åƒçš„å¤§å°
    """
    # è®¾ç½®ä¸­æ–‡å­—ä½“
    plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei', 'Arial Unicode MS']
    plt.rcParams['axes.unicode_minus'] = False
    
    # æ•´ç†æ•°æ®
    df = pd.DataFrame(results_dict).T
    df = df.fillna(0)  # å¡«å…… NaN å€¼
    
    # å®šä¹‰æŒ‡æ ‡æ–¹å‘ï¼šTrueè¡¨ç¤ºè¶Šå¤§è¶Šå¥½ï¼ŒFalseè¡¨ç¤ºè¶Šå°è¶Šå¥½
    metric_directions = {
        'micro_f1': True,
        'macro_f1': True, 
        'subset_accuracy': True,
        'macro_auc': True,
        'hamming_loss': False  # hamming loss è¶Šå°è¶Šå¥½
    }
    
    # ç»Ÿä¸€åˆ—åæ˜ å°„ï¼Œå…¼å®¹ä¸åŒçš„å‘½åçº¦å®š
    column_mapping = {
        'hamming_loss': 'hamming',
        'subset_accuracy': 'subset_acc', 
        'macro_auc': 'auc'
    }
    
    # é‡å‘½ååˆ—ä»¥ä¿æŒä¸€è‡´æ€§
    df_renamed = df.rename(columns=column_mapping)
    
    # ==================== å›¾ç»„ 1: å„æŒ‡æ ‡è¯¦ç»†å¯¹æ¯”ï¼ˆæŒ‰æŒ‡æ ‡åˆ†ç»„å±•ç¤ºå„æ¨¡å‹è¡¨ç°ï¼‰ ====================
    # ç›®æ ‡ï¼šæ¯ä¸ªæŒ‡æ ‡ä¸ºä¸€ä¸ªç»„ï¼Œç»„å†…å¹¶æ’æ˜¾ç¤ºä¸åŒæ¨¡å‹çš„æŸ±çŠ¶å€¼ï¼Œä¾¿äºæ¯”è¾ƒåŒä¸€æŒ‡æ ‡ä¸‹çš„æ¨¡å‹è¡¨ç°
    metrics_bar = [m for m in ['micro_f1', 'macro_f1', 'subset_acc', 'auc', 'hamming'] if m in df_renamed.columns]
    if len(metrics_bar) == 0:
        print("No metrics available for bar chart.")
    else:
        fig1, ax1 = plt.subplots(1, 1, figsize=figsize)
        df_bar = df_renamed[metrics_bar].copy()  # shape: (n_models, n_metrics)

        n_models = df_bar.shape[0]
        n_metrics = df_bar.shape[1]
        x = np.arange(n_metrics)
        total_width = 0.8
        width = total_width / max(n_models, 1)

        colors = plt.cm.tab20(np.linspace(0, 1, n_models))
        for i, model_name in enumerate(df_bar.index):
            values = df_bar.loc[model_name].values
            ax1.bar(x + i * width - total_width/2 + width/2, values, width, label=model_name, color=colors[i])
            # annotate
            for xi, v in zip(x + i * width - total_width/2 + width/2, values):
                ax1.text(xi, v + 0.01, f'{v:.3f}', ha='center', va='bottom', fontsize=8)

        ax1.set_xticks(x)
        ax1.set_xticklabels([m.replace('_', ' ').title() for m in metrics_bar])
        ax1.set_ylim(0, 1.05)
        ax1.set_ylabel('åˆ†æ•°')
        ax1.set_title('å„æŒ‡æ ‡è¯¦ç»†å¯¹æ¯”ï¼ˆæŒ‰æŒ‡æ ‡åˆ†ç»„ï¼Œç»„å†…ä¸ºä¸åŒæ¨¡å‹ï¼‰', fontweight='bold')
        ax1.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
        ax1.grid(axis='y', alpha=0.3)

        plt.tight_layout()
        if save_prefix:
            save_path1 = f"{save_prefix}_metrics_grouped.png"
            plt.savefig(save_path1, dpi=300, bbox_inches='tight')
            print(f"å„æŒ‡æ ‡å¯¹æ¯”æŸ±çŠ¶å›¾å·²ä¿å­˜åˆ°: {save_path1}")
        plt.show()
    
    # ==================== å›¾ç»„ 2: æ’åçƒ­åŠ›å›¾ + ç»¼åˆæ’å ====================
    fig2, (ax3, ax4) = plt.subplots(1, 2, figsize=figsize)
    
    # 2.1 æ’åçƒ­åŠ›å›¾
    ranking_data = {}
    # è¶Šå¤§è¶Šå¥½çš„æŒ‡æ ‡
    for metric in ['micro_f1', 'macro_f1', 'subset_acc', 'auc']:
        if metric in df_renamed.columns:
            ranking_data[metric] = df_renamed[metric].rank(ascending=False, method='min').astype(int)
    
    # è¶Šå°è¶Šå¥½çš„æŒ‡æ ‡ (hamming loss)
    if 'hamming' in df_renamed.columns:
        ranking_data['hamming'] = df_renamed['hamming'].rank(ascending=True, method='min').astype(int)
    
    if ranking_data:
        ranking_df = pd.DataFrame(ranking_data)
        
        sns.heatmap(ranking_df, annot=True, fmt='d', cmap='RdYlGn_r', 
                    ax=ax3, cbar_kws={'label': 'æ’å'})
        ax3.set_title('å„æŒ‡æ ‡æ¨¡å‹æ’å (1=æœ€ä½³)', fontweight='bold')
        ax3.set_xlabel('è¯„ä¼°æŒ‡æ ‡')
        ax3.set_ylabel('æ¨¡å‹')
        
        # 2.2 ç»¼åˆæ’å
        avg_rank = ranking_df.mean(axis=1).sort_values()
        
        bars = ax4.barh(range(len(avg_rank)), avg_rank.values)
        ax4.set_yticks(range(len(avg_rank)))
        ax4.set_yticklabels(avg_rank.index)
        ax4.set_xlabel('å¹³å‡æ’å')
        ax4.set_title('ç»¼åˆæ’å\n(å¹³å‡æ’åè¶Šå°è¶Šå¥½)', fontweight='bold')
        ax4.grid(True, alpha=0.3)
        
        # æ·»åŠ æ•°å€¼æ ‡ç­¾å’Œé¢œè‰²
        best_idx = avg_rank.index[0]
        worst_idx = avg_rank.index[-1]
        for i, (model, rank) in enumerate(avg_rank.items()):
            width = bars[i].get_width()
            ax4.text(width + 0.05, bars[i].get_y() + bars[i].get_height()/2, 
                    f'{width:.2f}', ha='left', va='center')
            
            if model == best_idx:
                bars[i].set_color('green')
                bars[i].set_alpha(0.7)
            elif model == worst_idx:
                bars[i].set_color('red')
                bars[i].set_alpha(0.7)
    
    plt.tight_layout()
    if save_prefix:
        save_path2 = f"{save_prefix}_ranking.png"
        plt.savefig(save_path2, dpi=300, bbox_inches='tight')
        print(f"æ’åå›¾å·²ä¿å­˜åˆ°: {save_path2}")
    plt.show()
    
    # ==================== å›¾ç»„ 3: æŒ‡æ ‡åˆ†å¸ƒç®±çº¿å›¾ ====================
    fig3, ax5 = plt.subplots(1, 1, figsize=(figsize[0], figsize[1]//2))
    
    # å‡†å¤‡ç®±çº¿å›¾æ•°æ®
    plot_data = []
    plot_labels = []
    
    for metric in ['micro_f1', 'macro_f1', 'subset_acc', 'auc']:
        if metric in df_renamed.columns:
            plot_data.append(df_renamed[metric].values)
            plot_labels.append(metric.replace('_', ' ').title())
    
    if plot_data:
        bp = ax5.boxplot(plot_data, labels=plot_labels, patch_artist=True)
        
        # è®¾ç½®ç®±çº¿å›¾é¢œè‰²
        colors = ['lightblue', 'lightgreen', 'lightcoral', 'lightyellow']
        for patch, color in zip(bp['boxes'], colors[:len(plot_data)]):
            patch.set_facecolor(color)
        
        ax5.set_title('å„æŒ‡æ ‡æ•°å€¼åˆ†å¸ƒ', fontweight='bold')
        ax5.set_ylabel('åˆ†æ•°')
        ax5.grid(True, alpha=0.3)
        ax5.set_ylim(0, 1)
    
    plt.tight_layout()
    if save_prefix:
        save_path3 = f"{save_prefix}_distribution.png"  
        plt.savefig(save_path3, dpi=300, bbox_inches='tight')
        print(f"åˆ†å¸ƒå›¾å·²ä¿å­˜åˆ°: {save_path3}")
    plt.show()
    
    # æ‰“å°æ€»ç»“
    if 'ranking_df' in locals() and 'avg_rank' in locals():
        print("\n" + "="*60)
        print("æ¨¡å‹æ€§èƒ½æ€»ç»“")
        print("="*60)
        print("ç»¼åˆæ’å (å¹³å‡æ’åè¶Šå°è¶Šå¥½):")
        for i, (model, rank) in enumerate(avg_rank.items(), 1):
            print(f"{i:2d}. {model:<20} å¹³å‡æ’å: {rank:.2f}")
        
        print(f"\nğŸ† æœ€ä½³æ¨¡å‹: {avg_rank.index[0]}")
        print(f"ğŸ“Š æ€§èƒ½è¯¦æƒ…:")
        best_model_metrics = df_renamed.loc[avg_rank.index[0]]
        for metric, value in best_model_metrics.items():
            if not np.isnan(value):
                direction = "â†‘" if metric_directions.get(metric, True) else "â†“"
                print(f"   {metric}: {value:.4f} {direction}")
        
        return df_renamed, ranking_df, avg_rank
    else:
        return df_renamed, None, None

def main():
    # ================= é…ç½®å‚æ•° =================
    enable_threshold_optimization = False  # æ˜¯å¦å¯ç”¨é˜ˆå€¼ä¼˜åŒ–
    threshold_validation_split = 0.5    # ç”¨äºé˜ˆå€¼ä¼˜åŒ–çš„éªŒè¯é›†æ¯”ä¾‹
    threshold_method = 'f1'               # é˜ˆå€¼ä¼˜åŒ–ç›®æ ‡: 'f1' æˆ– 'balanced_accuracy'
    
    # ç”Ÿæˆç¤ºä¾‹å¤šæ ‡ç­¾æ•°æ®
    # X, Y = make_multilabel_classification(n_samples=200, n_features=20, n_classes=6,
    #                                       n_labels=2, random_state=0)
    # X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)
    X_train, X_test, y_train, y_test = get_dataset()
    n_labels = y_train.shape[1]
    n_features = X_train.shape[1]
    
    print(f"é˜ˆå€¼ä¼˜åŒ–: {'å¯ç”¨' if enable_threshold_optimization else 'ç¦ç”¨'}")
    if enable_threshold_optimization:
        print(f"ä¼˜åŒ–æ–¹æ³•: {threshold_method}, éªŒè¯é›†æ¯”ä¾‹: {threshold_validation_split}")
        # ä¸ºé˜ˆå€¼ä¼˜åŒ–åˆ†å‰²éªŒè¯é›†
        X_test, X_val_threshold, y_test, y_val_threshold = train_test_split(
            X_test, y_test, test_size=threshold_validation_split, random_state=42
        )
    else:
        X_val_threshold, y_val_threshold = None, None
    
    model_dict = {
        'Logistic': LogisticRegression(max_iter=1000),
        'RandomForest': RandomForestClassifier(
            n_estimators=200,
            max_depth=12,
            n_jobs=-1,
            max_features='sqrt',
            min_samples_split=4,
            min_samples_leaf=2,
            class_weight='balanced_subsample',
            random_state=42
        ),
        'XGBoost': XGBClassifier(
            n_estimators=1000,
            max_depth=6,
            learning_rate=0.1,
            subsample=0.8,
            colsample_bytree=0.8,
            eval_metric='logloss',
            n_jobs=-1,
            random_state=42,
            tree_method='hist',
            # device='cuda',
        ),
        'TabPFN': TabPFNClassifier(),
    }
    
    BR_results = {}
    CC_results = {}
    MLP_results = {}
    for name, model in model_dict.items():
        print(f"\nè®­ç»ƒ BR+{name}...")
        br_model = MultiOutputClassifier(model, n_jobs=-1)
        br_model.fit(X_train, y_train)
        
        if hasattr(br_model, "predict_proba"):
            probas = br_model.predict_proba(X_test)
            # ä¼ å…¥æ¯ä¸ªå­åˆ†ç±»å™¨çš„ classes_ ä»¥ä¾¿å‡†ç¡®å®šä½æ­£ç±»åˆ—
            classes_list = []
            if hasattr(br_model, "estimators_"):
                for est in br_model.estimators_:
                    classes_list.append(getattr(est, "classes_", None))
            probas_mat = formalize_output_probas(probas, n_labels, classes_list=classes_list)
        else:
            probas_mat = br_model.predict(X_test).astype(float)
        
        # é˜ˆå€¼ä¼˜åŒ–
        optimal_thresholds = None
        if enable_threshold_optimization and hasattr(br_model, "predict_proba"):
            # åœ¨éªŒè¯é›†ä¸Šè·å–é¢„æµ‹æ¦‚ç‡
            val_probas = br_model.predict_proba(X_val_threshold)
            val_probas_mat = formalize_output_probas(val_probas, n_labels, classes_list=classes_list)
            
            # ä¼˜åŒ–é˜ˆå€¼
            optimal_thresholds, threshold_scores = optimize_thresholds(
                y_val_threshold, val_probas_mat, method=threshold_method, verbose=False
            )
            print(f"  ä¼˜åŒ–é˜ˆå€¼: {optimal_thresholds}")
        
        BR_results[name] = evaluate_multilabel(y_test, probas_mat, obj_info=f"BR+{name}", thresholds=optimal_thresholds)
        
    for name, model in model_dict.items():
        if name == 'TabPFN': continue
        print(f"\nè®­ç»ƒ CC+{name}...")
        chain = ClassifierChain(model, order='random', random_state=42)
        chain.fit(X_train, y_train)
        
        if hasattr(chain, "predict_proba"):
            try:
                probas_chain = chain.predict_proba(X_test)
                # chain.estimators_ é¡ºåºå¯¹åº”æ¯ä¸ªæ ‡ç­¾çš„å­æ¨¡å‹ï¼Œä¼ å…¥ classes_ æœ‰åŠ©äºç²¾ç¡®æŠ½å–
                classes_list = []
                if hasattr(chain, "estimators_"):
                    classes_list = classes_list_from_chain(chain, n_labels)
                probas_chain_mat = formalize_output_probas(probas_chain, n_labels, classes_list=classes_list)
            except Exception:
                # fallback: æ‰‹å·¥æŒ‰é¡ºåºç”¨ estimators_ é€æ­¥æ„é€ æ¦‚ç‡æˆ–ä»¥ç¡¬é¢„æµ‹ä½œä¸ºè¿‘ä¼¼
                X_aug = X_test.copy()
                cols = []
                for est in chain.estimators_:
                    p = est.predict_proba(X_aug)
                    if p.ndim == 2 and p.shape[1] > 1:
                        pos = p[:, 1]
                    else:
                        pos = p.ravel()
                    cols.append(pos)
                    # append hard prediction as next feature for chain (greedy)
                    X_aug = np.hstack([X_aug, (pos > 0.5).astype(int)[:, None]])
                probas_chain_mat = np.column_stack(cols)
        else:
            # æ²¡æœ‰ predict_probaï¼Œé€€å›åˆ° predict çš„ 0/1
            probas_chain_mat = chain.predict(X_test).astype(float)
        
        # é˜ˆå€¼ä¼˜åŒ–
        optimal_thresholds = None
        if enable_threshold_optimization and hasattr(chain, "predict_proba"):
            try:
                # åœ¨éªŒè¯é›†ä¸Šè·å–é¢„æµ‹æ¦‚ç‡
                val_probas_chain = chain.predict_proba(X_val_threshold)
                val_classes_list = []
                if hasattr(chain, "estimators_"):
                    val_classes_list = classes_list_from_chain(chain, n_labels)
                val_probas_chain_mat = formalize_output_probas(val_probas_chain, n_labels, classes_list=val_classes_list)
                
                # ä¼˜åŒ–é˜ˆå€¼
                optimal_thresholds, threshold_scores = optimize_thresholds(
                    y_val_threshold, val_probas_chain_mat, method=threshold_method, verbose=False
                )
                print(f"  ä¼˜åŒ–é˜ˆå€¼: {optimal_thresholds}")
            except Exception as e:
                print(f"  é˜ˆå€¼ä¼˜åŒ–å¤±è´¥: {e}")
                optimal_thresholds = None
        
        CC_results[name] = evaluate_multilabel(y_test, probas_chain_mat, obj_info=f"CC+{name}", thresholds=optimal_thresholds)
        
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"\nè®­ç»ƒ MLP...")
    model = MLPBaseline(
        n_features=n_features,
        n_labels=n_labels,
    ).to(device)
    optimizer = torch.optim.AdamW(model.parameters(), lr=3e-3, weight_decay=1e-3)
    criterion = nn.BCEWithLogitsLoss()
    
    train(
        model = model,
        X_train=X_train,
        y_train=y_train,
        optimizer=optimizer,
        criterion=criterion,
        epochs=20,
        batch_size=64
    )
    
    model.eval()
    with torch.no_grad():
        X_t= torch.from_numpy(X_test).to(device, dtype=next(model.parameters()).dtype)
        logits = model(X_t)
        probs = torch.sigmoid(logits).cpu().numpy()
        
        # MLPé˜ˆå€¼ä¼˜åŒ–
        optimal_thresholds = None
        if enable_threshold_optimization:
            # åœ¨éªŒè¯é›†ä¸Šè·å–é¢„æµ‹æ¦‚ç‡
            X_val_t = torch.from_numpy(X_val_threshold).to(device, dtype=next(model.parameters()).dtype)
            val_logits = model(X_val_t)
            val_probs = torch.sigmoid(val_logits).cpu().numpy()
            
            # ä¼˜åŒ–é˜ˆå€¼
            optimal_thresholds, threshold_scores = optimize_thresholds(
                y_val_threshold, val_probs, method=threshold_method, verbose=False
            )
            print(f"  ä¼˜åŒ–é˜ˆå€¼: {optimal_thresholds}")
        
        MLP_results['MLP'] = evaluate_multilabel(y_test, probs, obj_info=f"MLP", thresholds=optimal_thresholds)
        
    # TabPFN + Attention æ¨¡å‹ï¼ˆå¦‚æœå­˜åœ¨ä¿å­˜çš„æ¨¡å‹ï¼‰
    try:
        print(f"\nåŠ è½½ TabPFN+Attention...")
        tabpfn_att = MLPWithAttention(n_labels=n_labels)
        model_path = 'models/model_early_stop_epoch16_0907_1931.pt'
        load_model(model=tabpfn_att, path=model_path)
        probas = predict(model=tabpfn_att, X_train=X_train, y_train=y_train, X_test=X_test)
        
        # TabPFN+Attentioné˜ˆå€¼ä¼˜åŒ–
        optimal_thresholds = None
        if enable_threshold_optimization:
            # åœ¨éªŒè¯é›†ä¸Šè·å–é¢„æµ‹æ¦‚ç‡
            val_probas = predict(model=tabpfn_att, X_train=X_train, y_train=y_train, X_test=X_val_threshold)
            
            # ä¼˜åŒ–é˜ˆå€¼
            optimal_thresholds, threshold_scores = optimize_thresholds(
                y_val_threshold, val_probas, method=threshold_method, verbose=False
            )
            print(f"  ä¼˜åŒ–é˜ˆå€¼: {optimal_thresholds}")
        
        MLP_results['TabPFN+Attention'] = evaluate_multilabel(y_test, probas, obj_info="TabPFN+Attention", thresholds=optimal_thresholds)
    except Exception as e:
        print(f"æ— æ³•åŠ è½½ TabPFN+Attention æ¨¡å‹: {e}")
        print("è·³è¿‡ TabPFN+Attention è¯„ä¼°")
    
    # æ•´åˆæ‰€æœ‰ç»“æœ
    all_results = {}
    for name, result in BR_results.items():
        all_results[f'BR+{name}'] = result
    for name, result in CC_results.items():
        all_results[f'CC+{name}'] = result
    for name, result in MLP_results.items():
        all_results[name] = result
    
    # å¯è§†åŒ–å¯¹æ¯”
    print("\n" + "="*80)
    print("å¼€å§‹å¯è§†åŒ–æ¨¡å‹å¯¹æ¯”...")
    print("="*80)
    
    timestamp = datetime.now().strftime("%m%d_%H%M")
    
    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    import os
    os.makedirs('plots', exist_ok=True)
    
    visualize_model_comparison(
        all_results, 
        save_prefix=f'plots/model_comparison_{timestamp}',
        figsize=(16, 10)
    )

if __name__ == "__main__":
    main()