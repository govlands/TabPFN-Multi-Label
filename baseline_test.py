import numpy as np
from sklearn.datasets import make_multilabel_classification
from sklearn.model_selection import train_test_split
from sklearn.multioutput import MultiOutputClassifier, ClassifierChain
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier
from tabpfn import TabPFNClassifier
import torch
import torch.nn as nn
from mlp_att import evaluate_multilabel, get_dataset, MultiLabelTabPFN_LabelOnly, train, MLPWithAttention, predict, load_model
from feature_label_attn import MultiLabelTabPFN_FeatureLabel, predict_joint, JointFeatureLabelAttn
from joint_training import MultiLabelTabPFN_e2eFeatureLabel

from utils import formalize_output_probas
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
from datetime import datetime

def classes_list_from_chain(chain, n_labels):
    """
    è¿”å›é•¿åº¦ä¸º n_labels çš„åˆ—è¡¨ï¼Œç´¢å¼•å¯¹åº”åŸå§‹æ ‡ç­¾åˆ—ï¼Œ
    æ¯é¡¹ä¸ºè¯¥æ ‡ç­¾å¯¹åº”å­åˆ†ç±»å™¨çš„ classes_ï¼ˆæˆ–è€… Noneï¼‰ã€‚
    """
    classes_list = [None] * n_labels
    if not hasattr(chain, "estimators_"):
        return classes_list

    # chain.order_ ç»™å‡º estimators_ å¯¹åº”çš„åŸå§‹æ ‡ç­¾ç´¢å¼•é¡ºåº
    order = getattr(chain, "order_", None)
    if order is None:
        # sklearn æ—§ç‰ˆæœ¬å¯èƒ½æ²¡æœ‰ order_ï¼Œå‡å®šæŒ‰ 0..n_labels-1
        order = np.arange(n_labels)

    for est, lbl_idx in zip(chain.estimators_, order):
        classes_list[int(lbl_idx)] = getattr(est, "classes_", None)
    return classes_list

class MLPBaseline(nn.Module):
    def __init__(self, n_features, n_labels, hidden=(256, 128), p_drop=0.2, use_layernorm=True):
        super().__init__()
        layers = []
        d_prev = n_features
        for d_h in hidden:
            layers.append(nn.Linear(d_prev, d_h))
            layers.append(nn.LayerNorm(d_h) if use_layernorm else nn.BatchNorm1d(d_h))
            layers.append(nn.GELU())
            layers.append(nn.Dropout(p_drop))
            d_prev = d_h
        self.backbone = nn.Sequential(*layers)
        self.head = nn.Linear(d_prev, n_labels)
        
    def forward(self, X):
        # X:(batch, n_features)
        z = self.backbone(X)
        return self.head(z)

def visualize_model_comparison(results_dict, save_prefix=None, figsize=(16, 10)):
    """
    å¯¹æ¯”å¤šä¸ªæ¨¡å‹çš„æ€§èƒ½æŒ‡æ ‡ï¼Œåˆ†ç»„æ˜¾ç¤ºå¹¶ä¿å­˜å¤šä¸ªå›¾åƒæ–‡ä»¶
    
    å‚æ•°:
        results_dict: dictï¼Œæ ¼å¼ {model_name: {metric_name: value, ...}, ...}
        save_prefix: strï¼Œä¿å­˜å›¾åƒçš„å‰ç¼€è·¯å¾„ï¼ˆå¯é€‰ï¼‰
        figsize: tupleï¼Œæ¯ç»„å›¾åƒçš„å¤§å°
    """
    # è®¾ç½®ä¸­æ–‡å­—ä½“
    plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei', 'Arial Unicode MS']
    plt.rcParams['axes.unicode_minus'] = False
    
    # æ•´ç†æ•°æ®
    df = pd.DataFrame(results_dict).T
    df = df.fillna(0)  # å¡«å…… NaN å€¼
    
    # å®šä¹‰æŒ‡æ ‡æ–¹å‘ï¼šTrueè¡¨ç¤ºè¶Šå¤§è¶Šå¥½ï¼ŒFalseè¡¨ç¤ºè¶Šå°è¶Šå¥½
    metric_directions = {
        'micro_f1': True,
        'macro_f1': True, 
        'subset_accuracy': True,
        'macro_auc': True,
        'hamming_loss': False  # hamming loss è¶Šå°è¶Šå¥½
    }
    
    # ç»Ÿä¸€åˆ—åæ˜ å°„ï¼Œå…¼å®¹ä¸åŒçš„å‘½åçº¦å®š
    column_mapping = {
        'hamming_loss': 'hamming',
        'subset_accuracy': 'subset_acc', 
        'macro_auc': 'auc'
    }
    
    # é‡å‘½ååˆ—ä»¥ä¿æŒä¸€è‡´æ€§
    df_renamed = df.rename(columns=column_mapping)
    
    # ==================== å›¾ç»„ 1: å„æŒ‡æ ‡è¯¦ç»†å¯¹æ¯”ï¼ˆæŒ‰æŒ‡æ ‡åˆ†ç»„å±•ç¤ºå„æ¨¡å‹è¡¨ç°ï¼‰ ====================
    # ç›®æ ‡ï¼šæ¯ä¸ªæŒ‡æ ‡ä¸ºä¸€ä¸ªç»„ï¼Œç»„å†…å¹¶æ’æ˜¾ç¤ºä¸åŒæ¨¡å‹çš„æŸ±çŠ¶å€¼ï¼Œä¾¿äºæ¯”è¾ƒåŒä¸€æŒ‡æ ‡ä¸‹çš„æ¨¡å‹è¡¨ç°
    # å°†hamming lossåˆ†ç¦»åˆ°å•ç‹¬çš„å›¾ä¸­ï¼Œå…¶ä»–æŒ‡æ ‡æ”¾åœ¨ä¸»å›¾ä¸­
    
    # ä¸»è¦æŒ‡æ ‡ï¼ˆè¶Šé«˜è¶Šå¥½ï¼‰
    main_metrics = [m for m in ['micro_f1', 'macro_f1', 'subset_acc', 'auc'] if m in df_renamed.columns]
    # hamming lossï¼ˆè¶Šä½è¶Šå¥½ï¼‰
    hamming_metrics = [m for m in ['hamming'] if m in df_renamed.columns]
    
    if len(main_metrics) == 0 and len(hamming_metrics) == 0:
        print("No metrics available for bar chart.")
    else:
        # åˆ›å»ºå­å›¾ï¼šç¬¬ä¸€ä¸ªç”¨äºä¸»è¦æŒ‡æ ‡ï¼Œç¬¬äºŒä¸ªç”¨äºhamming loss
        if len(hamming_metrics) > 0:
            fig1, (ax1, ax2) = plt.subplots(1, 2, figsize=(figsize[0] * 1.5, figsize[1]))
        else:
            fig1, ax1 = plt.subplots(1, 1, figsize=figsize)
            ax2 = None
        
        n_models = df_renamed.shape[0]
        colors = plt.cm.tab20(np.linspace(0, 1, n_models))
        
        # ======== ä¸»è¦æŒ‡æ ‡å›¾ ========
        if len(main_metrics) > 0:
            df_main = df_renamed[main_metrics].copy()
            n_metrics = df_main.shape[1]
            x = np.arange(n_metrics)
            total_width = 0.8
            width = total_width / max(n_models, 1)
            
            for i, model_name in enumerate(df_main.index):
                values = df_main.loc[model_name].values
                ax1.bar(x + i * width - total_width/2 + width/2, values, width, label=model_name, color=colors[i])
                # annotate
                for xi, v in zip(x + i * width - total_width/2 + width/2, values):
                    ax1.text(xi, v + 0.005, f'{v:.3f}', ha='center', va='bottom', fontsize=8)
            
            ax1.set_xticks(x)
            ax1.set_xticklabels([m.replace('_', ' ').title() for m in main_metrics])
            # è°ƒæ•´çºµè½´èŒƒå›´ä»¥å¢å¼ºè§†è§‰å·®å¼‚ - ä¸»è¦æŒ‡æ ‡é€šå¸¸åœ¨0.3-0.9ä¹‹é—´
            main_values = df_main.values
            min_val = max(0, main_values.min() - 0.05)
            max_val = min(1.0, main_values.max() + 0.05)
            ax1.set_ylim(min_val, max_val)
            ax1.set_ylabel('åˆ†æ•°ï¼ˆè¶Šé«˜è¶Šå¥½ï¼‰')
            ax1.set_title('ä¸»è¦æŒ‡æ ‡å¯¹æ¯”', fontweight='bold')
            ax1.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
            ax1.grid(axis='y', alpha=0.3)
        
        # ======== Hamming Loss å›¾ ========
        if len(hamming_metrics) > 0 and ax2 is not None:
            df_hamming = df_renamed[hamming_metrics].copy()
            n_metrics_h = df_hamming.shape[1]
            x_h = np.arange(n_metrics_h)
            total_width_h = 0.8
            width_h = total_width_h / max(n_models, 1)
            
            for i, model_name in enumerate(df_hamming.index):
                values_h = df_hamming.loc[model_name].values
                ax2.bar(x_h + i * width_h - total_width_h/2 + width_h/2, values_h, width_h, label=model_name, color=colors[i])
                # annotate
                for xi, v in zip(x_h + i * width_h - total_width_h/2 + width_h/2, values_h):
                    ax2.text(xi, v + 0.002, f'{v:.3f}', ha='center', va='bottom', fontsize=8)
            
            ax2.set_xticks(x_h)
            ax2.set_xticklabels([m.replace('_', ' ').title() for m in hamming_metrics])
            # è°ƒæ•´çºµè½´èŒƒå›´ä»¥å¢å¼ºè§†è§‰å·®å¼‚ - hamming lossé€šå¸¸åœ¨0.1-0.4ä¹‹é—´
            hamming_values = df_hamming.values
            min_val_h = max(0, hamming_values.min() - 0.02)
            max_val_h = hamming_values.max() + 0.02
            ax2.set_ylim(min_val_h, max_val_h)
            ax2.set_ylabel('åˆ†æ•°ï¼ˆè¶Šä½è¶Šå¥½ï¼‰')
            ax2.set_title('Hamming Loss å¯¹æ¯”', fontweight='bold')
            ax2.grid(axis='y', alpha=0.3)
        
        plt.tight_layout()
        if save_prefix:
            save_path1 = f"{save_prefix}_metrics_grouped.png"
            plt.savefig(save_path1, dpi=300, bbox_inches='tight')
            print(f"å„æŒ‡æ ‡å¯¹æ¯”æŸ±çŠ¶å›¾å·²ä¿å­˜åˆ°: {save_path1}")
        plt.show()
    
    # ==================== å›¾ç»„ 2: æ’åçƒ­åŠ›å›¾ + ç»¼åˆæ’å ====================
    fig2, (ax3, ax4) = plt.subplots(1, 2, figsize=figsize)
    
    # 2.1 æ’åçƒ­åŠ›å›¾
    ranking_data = {}
    # è¶Šå¤§è¶Šå¥½çš„æŒ‡æ ‡
    for metric in ['micro_f1', 'macro_f1', 'subset_acc', 'auc']:
        if metric in df_renamed.columns:
            ranking_data[metric] = df_renamed[metric].rank(ascending=False, method='min').astype(int)
    
    # è¶Šå°è¶Šå¥½çš„æŒ‡æ ‡ (hamming loss)
    if 'hamming' in df_renamed.columns:
        ranking_data['hamming'] = df_renamed['hamming'].rank(ascending=True, method='min').astype(int)
    
    if ranking_data:
        ranking_df = pd.DataFrame(ranking_data)
        
        sns.heatmap(ranking_df, annot=True, fmt='d', cmap='RdYlGn_r', 
                    ax=ax3, cbar_kws={'label': 'æ’å'})
        ax3.set_title('å„æŒ‡æ ‡æ¨¡å‹æ’å (1=æœ€ä½³)', fontweight='bold')
        ax3.set_xlabel('è¯„ä¼°æŒ‡æ ‡')
        ax3.set_ylabel('æ¨¡å‹')
        
        # 2.2 ç»¼åˆæ’å
        avg_rank = ranking_df.mean(axis=1).sort_values()
        
        bars = ax4.barh(range(len(avg_rank)), avg_rank.values)
        ax4.set_yticks(range(len(avg_rank)))
        ax4.set_yticklabels(avg_rank.index)
        ax4.set_xlabel('å¹³å‡æ’å')
        ax4.set_title('ç»¼åˆæ’å\n(å¹³å‡æ’åè¶Šå°è¶Šå¥½)', fontweight='bold')
        ax4.grid(True, alpha=0.3)
        
        # æ·»åŠ æ•°å€¼æ ‡ç­¾å’Œé¢œè‰²
        best_idx = avg_rank.index[0]
        worst_idx = avg_rank.index[-1]
        for i, (model, rank) in enumerate(avg_rank.items()):
            width = bars[i].get_width()
            ax4.text(width + 0.05, bars[i].get_y() + bars[i].get_height()/2, 
                    f'{width:.2f}', ha='left', va='center')
            
            if model == best_idx:
                bars[i].set_color('green')
                bars[i].set_alpha(0.7)
            elif model == worst_idx:
                bars[i].set_color('red')
                bars[i].set_alpha(0.7)
    
    plt.tight_layout()
    if save_prefix:
        save_path2 = f"{save_prefix}_ranking.png"
        plt.savefig(save_path2, dpi=300, bbox_inches='tight')
        print(f"æ’åå›¾å·²ä¿å­˜åˆ°: {save_path2}")
    plt.show()
    
    # ==================== å›¾ç»„ 3: æŒ‡æ ‡åˆ†å¸ƒç®±çº¿å›¾ ====================
    fig3, ax5 = plt.subplots(1, 1, figsize=(figsize[0], figsize[1]//2))
    
    # å‡†å¤‡ç®±çº¿å›¾æ•°æ®
    plot_data = []
    plot_labels = []
    
    for metric in ['micro_f1', 'macro_f1', 'subset_acc', 'auc']:
        if metric in df_renamed.columns:
            plot_data.append(df_renamed[metric].values)
            plot_labels.append(metric.replace('_', ' ').title())
    
    if plot_data:
        bp = ax5.boxplot(plot_data, labels=plot_labels, patch_artist=True)
        
        # è®¾ç½®ç®±çº¿å›¾é¢œè‰²
        colors = ['lightblue', 'lightgreen', 'lightcoral', 'lightyellow']
        for patch, color in zip(bp['boxes'], colors[:len(plot_data)]):
            patch.set_facecolor(color)
        
        ax5.set_title('å„æŒ‡æ ‡æ•°å€¼åˆ†å¸ƒ', fontweight='bold')
        ax5.set_ylabel('åˆ†æ•°')
        ax5.grid(True, alpha=0.3)
        ax5.set_ylim(0, 1)
    
    plt.tight_layout()
    if save_prefix:
        save_path3 = f"{save_prefix}_distribution.png"  
        plt.savefig(save_path3, dpi=300, bbox_inches='tight')
        print(f"åˆ†å¸ƒå›¾å·²ä¿å­˜åˆ°: {save_path3}")
    plt.show()
    
    # æ‰“å°æ€»ç»“
    if 'ranking_df' in locals() and 'avg_rank' in locals():
        print("\n" + "="*60)
        print("æ¨¡å‹æ€§èƒ½æ€»ç»“")
        print("="*60)
        print("ç»¼åˆæ’å (å¹³å‡æ’åè¶Šå°è¶Šå¥½):")
        for i, (model, rank) in enumerate(avg_rank.items(), 1):
            print(f"{i:2d}. {model:<20} å¹³å‡æ’å: {rank:.2f}")
        
        print(f"\nğŸ† æœ€ä½³æ¨¡å‹: {avg_rank.index[0]}")
        print(f"ğŸ“Š æ€§èƒ½è¯¦æƒ…:")
        best_model_metrics = df_renamed.loc[avg_rank.index[0]]
        for metric, value in best_model_metrics.items():
            if not np.isnan(value):
                direction = "â†‘" if metric_directions.get(metric, True) else "â†“"
                print(f"   {metric}: {value:.4f} {direction}")
        
        return df_renamed, ranking_df, avg_rank
    else:
        return df_renamed, None, None

def main():
    # ç”Ÿæˆç¤ºä¾‹å¤šæ ‡ç­¾æ•°æ®
    # X, Y = make_multilabel_classification(n_samples=200, n_features=20, n_classes=6,
    #                                       n_labels=2, random_state=0)
    # X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)
    X_train, X_test, y_train, y_test = get_dataset()
    n_labels = y_train.shape[1]
    n_features = X_train.shape[1]
    
    model_dict = {
        'Logistic': LogisticRegression(max_iter=1000),
        'RandomForest': RandomForestClassifier(
            n_estimators=200,
            max_depth=12,
            n_jobs=16,
            max_features='sqrt',
            min_samples_split=4,
            min_samples_leaf=2,
            class_weight='balanced_subsample',
            random_state=42
        ),
        'XGBoost': XGBClassifier(
            n_estimators=1000,
            max_depth=6,
            learning_rate=0.1,
            subsample=0.8,
            colsample_bytree=0.8,
            eval_metric='logloss',
            n_jobs=16,
            random_state=42,
            tree_method='hist',
            # device='cuda',
        ),
        'TabPFN': TabPFNClassifier(),
    }
    
    BR_results = {}
    CC_results = {}
    MLP_results = {}
    for name, model in model_dict.items():
        print(f"\nè®­ç»ƒ BR+{name}...")
        br_model = MultiOutputClassifier(model, n_jobs=-1)
        br_model.fit(X_train, y_train)
        
        if hasattr(br_model, "predict_proba"):
            probas = br_model.predict_proba(X_test)
            # ä¼ å…¥æ¯ä¸ªå­åˆ†ç±»å™¨çš„ classes_ ä»¥ä¾¿å‡†ç¡®å®šä½æ­£ç±»åˆ—
            classes_list = []
            if hasattr(br_model, "estimators_"):
                for est in br_model.estimators_:
                    classes_list.append(getattr(est, "classes_", None))
            probas_mat = formalize_output_probas(probas, n_labels, classes_list=classes_list)
        else:
            probas_mat = br_model.predict(X_test).astype(float)
        
        BR_results[name] = evaluate_multilabel(y_test, probas_mat, obj_info=f"BR+{name}")
        
    for name, model in model_dict.items():
        if name == 'TabPFN': continue
        print(f"\nè®­ç»ƒ CC+{name}...")
        chain = ClassifierChain(model, order='random', random_state=42)
        chain.fit(X_train, y_train)
        
        if hasattr(chain, "predict_proba"):
            try:
                probas_chain = chain.predict_proba(X_test)
                # chain.estimators_ é¡ºåºå¯¹åº”æ¯ä¸ªæ ‡ç­¾çš„å­æ¨¡å‹ï¼Œä¼ å…¥ classes_ æœ‰åŠ©äºç²¾ç¡®æŠ½å–
                classes_list = []
                if hasattr(chain, "estimators_"):
                    classes_list = classes_list_from_chain(chain, n_labels)
                probas_chain_mat = formalize_output_probas(probas_chain, n_labels, classes_list=classes_list)
            except Exception:
                # fallback: æ‰‹å·¥æŒ‰é¡ºåºç”¨ estimators_ é€æ­¥æ„é€ æ¦‚ç‡æˆ–ä»¥ç¡¬é¢„æµ‹ä½œä¸ºè¿‘ä¼¼
                X_aug = X_test.copy()
                cols = []
                for est in chain.estimators_:
                    p = est.predict_proba(X_aug)
                    if p.ndim == 2 and p.shape[1] > 1:
                        pos = p[:, 1]
                    else:
                        pos = p.ravel()
                    cols.append(pos)
                    # append hard prediction as next feature for chain (greedy)
                    X_aug = np.hstack([X_aug, (pos > 0.5).astype(int)[:, None]])
                probas_chain_mat = np.column_stack(cols)
        else:
            # æ²¡æœ‰ predict_probaï¼Œé€€å›åˆ° predict çš„ 0/1
            probas_chain_mat = chain.predict(X_test).astype(float)
        
        CC_results[name] = evaluate_multilabel(y_test, probas_chain_mat, obj_info=f"CC+{name}")
        
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"\nè®­ç»ƒ MLP...")
    model = MLPBaseline(
        n_features=n_features,
        n_labels=n_labels,
    ).to(device)
    optimizer = torch.optim.AdamW(model.parameters(), lr=3e-3, weight_decay=1e-3)
    criterion = nn.BCEWithLogitsLoss()
    
    train(
        model = model,
        X_train=X_train,
        y_train=y_train,
        optimizer=optimizer,
        criterion=criterion,
        epochs=20,
        batch_size=64
    )
    
    model.eval()
    with torch.no_grad():
        X_t = torch.tensor(X_test, dtype=torch.float32, device=device)
        logits = model(X_t)
        probs = torch.sigmoid(logits).cpu().numpy()

        MLP_results['MLP'] = evaluate_multilabel(y_test, probs, obj_info=f"MLP")
        
    # TabPFN + Attention æ¨¡å‹ï¼ˆæ ‡ç­¾æ³¨æ„åŠ›ï¼‰
    try:
        print(f"\nè®­ç»ƒ TabPFN+Attention [labels only]...")
        # ä½¿ç”¨å°è£…çš„ç±»
        model_label_only = MultiLabelTabPFN_LabelOnly(
            n_labels=n_labels,
            epochs=20,
            batch_size=128,
            early_stopping_patience=5,
            validation_split=0.2,
        )
        model_label_only.fit(X_train, y_train, save_model=True)
        probas = model_label_only.predict_proba(X_test)
        
        MLP_results['TabPFN+Attention'] = evaluate_multilabel(y_test, probas, obj_info="TabPFN+Attention [labels only]")
        print("âœ… TabPFN+Attention [labels only] æ¨¡å‹å·²è®­ç»ƒå¹¶ä¿å­˜")
    except Exception as e:
        print(f"âŒ TabPFN+Attention [labels only] è®­ç»ƒå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

    try:
        print(f"\nè®­ç»ƒ TabPFN+Attention [features & labels]...")
        # ä½¿ç”¨å°è£…çš„ç±»
        model_feature_label = MultiLabelTabPFN_FeatureLabel(
            n_features=n_features,
            n_labels=n_labels,
            epochs=20,
            batch_size=128,
            early_stopping_patience=5,
            validation_split=0.15,
        )
        model_feature_label.fit(X_train, y_train, save_model=True)
        probas = model_feature_label.predict_proba(X_test)
        
        MLP_results['TabPFN+Attention joint'] = evaluate_multilabel(y_test, probas, obj_info="TabPFN+Attention [features & labels]")
        print("âœ… TabPFN+Attention [features & labels] æ¨¡å‹å·²è®­ç»ƒå¹¶ä¿å­˜")
    except Exception as e:
        print(f"âŒ TabPFN+Attention [features & labels] è®­ç»ƒå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

    try:
        print(f"\nè®­ç»ƒ TabPFN+Attention [e2e]...")
        # ä½¿ç”¨å°è£…çš„ç±» - ç«¯åˆ°ç«¯è®­ç»ƒ
        model_e2e = MultiLabelTabPFN_e2eFeatureLabel(
            n_features=n_features,
            n_labels=n_labels,
            epochs=10,  # è¾ƒå°‘çš„epochsé˜²æ­¢æ˜¾å­˜ä¸è¶³
            batch_size=400,
            n_splits=4,  # è¾ƒå°‘çš„splitså‡å°‘å†…å­˜å ç”¨
            pfn_n_estimators=4,  # å‡å°‘ä¼°è®¡å™¨æ•°é‡
            validation_split=0.15,
            early_stopping_patience=5,
            early_stopping_delta=1e-4,
        )
        model_e2e.fit(X_train, y_train)
        probas = model_e2e.predict_proba(X_test, mode='all')
        
        # ä¿å­˜æ¨¡å‹
        save_path = model_e2e.save()
        print(f"âœ… ç«¯åˆ°ç«¯æ¨¡å‹å·²ä¿å­˜åˆ°: {save_path}")
        
        MLP_results['TabPFN+Attention e2e'] = evaluate_multilabel(y_test, probas, obj_info="TabPFN+Attention [e2e]")
        print("âœ… TabPFN+Attention [e2e] æ¨¡å‹å·²è®­ç»ƒå¹¶ä¿å­˜")
    except Exception as e:
        print(f"âŒ TabPFN+Attention [e2e] è®­ç»ƒå¤±è´¥: {e}")
        print("è¿™å¯èƒ½æ˜¯ç”±äºæ˜¾å­˜ä¸è¶³å¯¼è‡´çš„ï¼Œè¯·è€ƒè™‘å‡å°‘batch_sizeæˆ–ä½¿ç”¨CPUè®­ç»ƒ")
        import traceback
        traceback.print_exc()
    
    # æ•´åˆæ‰€æœ‰ç»“æœ
    all_results = {}
    for name, result in BR_results.items():
        all_results[f'BR+{name}'] = result
    for name, result in CC_results.items():
        all_results[f'CC+{name}'] = result
    for name, result in MLP_results.items():
        all_results[name] = result
    
    # å¯è§†åŒ–å¯¹æ¯”
    print("\n" + "="*80)
    print("å¼€å§‹å¯è§†åŒ–æ¨¡å‹å¯¹æ¯”...")
    print("="*80)
    
    timestamp = datetime.now().strftime("%m%d_%H%M")
    
    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    import os
    os.makedirs('plots', exist_ok=True)
    
    visualize_model_comparison(
        all_results, 
        save_prefix=f'plots/model_comparison_{timestamp}',
        figsize=(16, 10)
    )

if __name__ == "__main__":
    main()